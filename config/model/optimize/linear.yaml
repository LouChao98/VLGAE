# @package _global_

optimizer:
  groups: []
  args:
    _target_: torch.optim.Adam
    lr: 1.0e-3
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.
    eps: 1.0e-12

scheduler:
  interval: step
  frequency: 1
  args:
    _target_: src.utility.scheduler.get_exponential_lr_scheduler
    gamma: 0.75**(1/2000)
